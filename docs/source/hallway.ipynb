{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc3ca63",
   "metadata": {},
   "source": [
    "# Hallway Environment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515e273",
   "metadata": {},
   "source": [
    "The `graphenv` library implements graph search as a reinforcement learning (RL) problem with parameteric action spaces, and is ready to use with many off-the-shelf algorithms avaialable in the RLLib framework.\n",
    "\n",
    "Before jumping into the implementation details, let's take a look a simple motivating example: the hallway problem.\n",
    "\n",
    "The hallway problem is effectively a 1d version of the gridworld problem in classic RL.  We are given a hallway with $N$ discrete positions and, starting at one end, want to learn to reach the opposite end in as few steps as possible.\n",
    "\n",
    "![hallway-flat](./img/hallway-flat.png)\n",
    "\n",
    "The figure above shows a hallway problem with $N=3$, and the optimal solution starting at state 0 and ending at state 2, with each \"current state\" highlighted in black.\n",
    "\n",
    "This trivial problem can be used to express the \"RL on a graph\" idea succinctly, and enable solving much more interesting, non-trivial problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc44a24",
   "metadata": {},
   "source": [
    "## The hallway problem as graph problem\n",
    "\n",
    "Before we jump into the graph formulation of the hallway problem, let's talk first about _actions_, because this is one of the key differences between `graphenv` and traditional RL gym environments.  \n",
    "\n",
    "Typically, discrete actions spaces in gym environments have fixed dimension and fixed definitions.  In the hallway problem, for instance, there are always two actions: \"move left\" (action=0) and \"move right\" (action=1).  Cases where the action is infeasible, like trying to move left from the start position, are typically handled by implmenting a _null_ transition where the action doesn't change the current state.\n",
    "\n",
    "In general graph search problems, however, such fixed action spaces are not practical.  In the game of chess, for example, the total number of possible board states and, subsequently, moves, is astronomical, while the set of _feasible_ moves changes continually throughout the game.\n",
    "\n",
    "The key observation that makes graph search tractable as a gym environment is:  even for large and complex graph search problems, the number of states that are accessible from the current state is usually relatively small.  If, instead of fixing the action to a pre-defined set of choices, we think of each action as representing an accessible next state, this endeavor becomes tractable.\n",
    "\n",
    "And so, we abandon the idea of \"fixed\" action spaces in favor of \"parametric\" action spaces.  Here, **parametric** means that actions coincide with next states represented by feature vectors, rather than having a single, index-based interpretation.  In other words, rather than actions as \"move $[ left, right ]$\", parametric actions can be thought of as \"go to state $[i, j]$\" where the states $i,j$ each have a vector representation.  Also unliked fixed spaces, the order doesn't matter:  we could equivalently say \"go to state $[j, i]$\".\n",
    "\n",
    "A key ingredient in making this machinery work is to have policy models that can work on parameteric action spaces\n",
    "\n",
    "The figure below illustrates how to think of the simple hallway example as a graph problem.  \n",
    "\n",
    "![hallway-graph](./img/hallway-graph.png)\n",
    "\n",
    "Before jumping into the specifics of how all of this works in `graphenv`, let's define some terms.\n",
    "\n",
    "A **vertex** reprents a single state in the graph which, for this problem, can be described by an index $i\\in \\{ 0, 1, 2 \\}$.  (Sometimes we'll use the terms vertex, state, and node interchangeably).  In the figure, each vertex is shown alongside the corresponding state of the hallway problem.\n",
    "\n",
    "The **root** is the starting vertex of the graph search, here, $i=0$.\n",
    "\n",
    "At each state in the search, a certain number of child states (or **children**) are accessible.  In the figure above, we illustrate this using the color codes:\n",
    "\n",
    "* black = current vertex\n",
    "* white = child vertex\n",
    "* gray = inaccessible, or **masked**, vertex\n",
    "\n",
    "If we think of an RL action as selecting one of these children, it's clear that the number of actions can change from one state to the next.  For example:\n",
    "\n",
    "* Starting at the root vertex $i=0$ (black), state $i=1$ (white) is accessible by moving right, while state $i=2$ (gray) can't be accessed in a single move.\n",
    "* Starting at vertex $i=1$ (black), both $i=0$ and $i=2$ are accessible (white) -- there are no masked states in this case.\n",
    "\n",
    "The **terminal** vertex here coincides with $i=2$.  Notice that this vertex has no children because, when reached, the problem is solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991cbf05",
   "metadata": {},
   "source": [
    "# The hallway problem as graphenv problem\n",
    "\n",
    "The graphenv module makes it easy for a user to implement their graph search problem as a gym environment, and then to plug that environment into RLLib using both custom and off-the-shelf RL algorithm.  At a high level, the user implements a `Vertex` and `Model` class to represent the graph state and correspnding RL policy model and graphenv takes care of the rest.\n",
    "\n",
    "The figure highlights below illustrates how the `Vertex` and `Model` classes interact, with data labeled on the left and associated methods labeled on the right.\n",
    "\n",
    "![graphenv](./img/graphenv.png)\n",
    "\n",
    "Below, we step through the implementation of the `HallwayState` (inheriting from the graphenv `Vertex`) and `HallwayModel` (inheriting from the graphenv `Model`).  We then provide a working example of building and running a hallway environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b82e2ef",
   "metadata": {},
   "source": [
    "## HallwayState\n",
    "\n",
    "(See `graphenv.examples.hallway.hallway_state` for the full implementation).\n",
    "\n",
    "The `HallwayState` represents all of the problem logic at the level of a single vertex that enables graphenv to automate the overarching search and RL training.  This class inherits from `graphenv.vertex.Vertex` which has a number of required methods and attributes which we step through below.\n",
    "\n",
    "### `__init__`\n",
    "\n",
    "As you'd expect, problem configuration happens here.  The hallway state is completely specified by the current and end positions,\n",
    "\n",
    "```python\n",
    "def __init__(\n",
    "        self,\n",
    "        corridor_length: int,\n",
    "        cur_pos: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes this HallwayState.\n",
    "        Args:\n",
    "            corridor_length (int): length of the vertex chain\n",
    "            cur_pos (int, optional): initial vertex index. Defaults to 0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.end_pos = corridor_length\n",
    "        self.cur_pos = cur_pos\n",
    "```\n",
    "\n",
    "### `observation_space`\n",
    "\n",
    "Returns a `gym.spaces.Space` object that describes the structure of the data used to represent a vertex.  In the hallway problem,\n",
    "\n",
    "```python\n",
    "@property\n",
    "def observation_space(self) -> gym.spaces.Dict:\n",
    "    \"\"\"HallwayStates are observed with a dictionary containing a single\n",
    "    key, 'cur_pos', with an integer value between 0 and self.end_pos,\n",
    "    indicating the index of the vertex.\n",
    "    Returns:\n",
    "        gym.spaces.Dict: The observation space for HallwayStates.\n",
    "    \"\"\"\n",
    "    return gym.spaces.Dict(\n",
    "        {\n",
    "            \"cur_pos\": gym.spaces.Box(\n",
    "                low=np.array([0]), high=np.array([self.end_pos]), dtype=int\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "where `cur_pos` is the integer index of the current position.  The box space has a single element containing the index of the current position but, in general, can contain multiple, complex subspaces.\n",
    "\n",
    "### `new`\n",
    "\n",
    "Returns a new vertex instance with updated state.  For the hallway problem, \n",
    "\n",
    "```python\n",
    "def new(self, cur_pos: int):\n",
    "    \"\"\"Convenience function for duplicating the existing node.\n",
    "    Returns:\n",
    "        HallwayState : a copy of this HallwayState.\n",
    "    \"\"\"\n",
    "    return HallwayState(self.end_pos, cur_pos)\n",
    "```\n",
    "\n",
    "where `cur_pos` is the position of the new state.\n",
    "\n",
    "\n",
    "### `root`\n",
    "\n",
    "Returns the root vertex.  In the hallway problem, we always go back to state 0,\n",
    "\n",
    "```python\n",
    "@property\n",
    "def root(self) -> \"HallwayState\":\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        HallwayState: initial state (vertex at index 0)\n",
    "    \"\"\"\n",
    "    return self.new(0)\n",
    "```\n",
    "\n",
    "### `reward`\n",
    "\n",
    "Returns the vertex reward.  For the hallway problem, we give a small negative reward for each non-terminal step, and a random, positive reward for reaching the goal.\n",
    "\n",
    "```python\n",
    "@property\n",
    "def reward(self) -> float:\n",
    "    \"\"\"The reward function for the HallwayState graph.\n",
    "    Returns:\n",
    "        float: random reward between 0 and 2 on the goal vertex, -0.1\n",
    "            otherwise.\n",
    "    \"\"\"\n",
    "    return random.random() * 2 if self.cur_pos >= self.end_pos else -0.1\n",
    "```\n",
    "\n",
    "### `_get_children`\n",
    "\n",
    "To take an action from a given vertex in the graph search, we need to be able observe its children.  The `Vertex` class implements this first part through a `_get_children` generator which, for the hallway problem, looks like:\n",
    "\n",
    "```python\n",
    "def _get_children(self) -> Sequence[\"HallwayState\"]:\n",
    "    \"\"\"Gets child verticies of this vertex. Each vertex has both larger\n",
    "    and smaller adjacent index verticies as children, except for the initial\n",
    "    and goal verticies.\n",
    "    Yields:\n",
    "        HallwayState: Child verticies of this vertex.\n",
    "    \"\"\"\n",
    "    if self.cur_pos < self.end_pos:\n",
    "        if self.cur_pos > 0:  # Stop the hallway from going negative\n",
    "            yield self.new(self.cur_pos - 1)\n",
    "        yield self.new(self.cur_pos + 1)\n",
    "```\n",
    "\n",
    "where the `new` methods simply returns a new instance with updated state index.\n",
    "\n",
    "In our example above, this method will yield \n",
    "\n",
    "```\n",
    "* [new(1)] if cur_pos == 0\n",
    "* [new(0), new(2)] if cur_pos == 1\n",
    "* [] if cur_pos == 2\n",
    "```\n",
    "\n",
    "Note that the number of children (actions) is variable, and that the terminal state returns an empty list of next children.\n",
    "\n",
    "\n",
    "### `_make_observation`\n",
    "\n",
    "To decide which child to transition to, the RL agent will need to call a policy model with that vertex's observation.  To this end, we implement `_make_observation` which, for the hallway example, returns:\n",
    "\n",
    "```python\n",
    "def _make_observation(self) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Makes an observation of this HallwayState vertex.\n",
    "    Returns:\n",
    "        Dict[str, np.ndarray]: dictionary containing the current position\n",
    "        index under the key 'cur_pos'.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"cur_pos\": np.array([self.cur_pos], dtype=int),\n",
    "    }\n",
    "```\n",
    "\n",
    "Note that the returned observation must exactly match the specification in the vertex's `observation_space`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f921544",
   "metadata": {},
   "source": [
    "## HallwayModel\n",
    "\n",
    "(See `graphenv.examples.hallway.hallway_model` for the full implementation).\n",
    "\n",
    "The `Model` class implements the policy model used by the RL algorithm and, as such, needs to be implemented to take vertex observation data as input, and to output an action value and action weight for each observation.  In practice, this amounts to implementing a keras model in the `__init__`, and storing it in the `base_model` attribute of the model class.\n",
    "\n",
    "```python\n",
    "class HallwayModel(GraphModel):\n",
    "    \"\"\"An example GraphModel implementation for the HallwayEnv and HallwayState\n",
    "    Graph.\n",
    "    Attributes:\n",
    "        base_model : The Keras model used to evaluate vertex observations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        hidden_dim: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Initializs this HallwayModel.\n",
    "        Uses a dense fully connected Keras network.\n",
    "        Args:\n",
    "            hidden_dim (int, optional): The number of hidden layers to use. \n",
    "                Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        cur_pos = layers.Input(shape=(1,), name=\"cur_pos\", dtype=tf.float32)\n",
    "\n",
    "        hidden_layer = layers.Dense(hidden_dim, name=\"hidden_layer\")\n",
    "        action_value_output = layers.Dense(\n",
    "            1, name=\"action_value_output\", bias_initializer=\"ones\"\n",
    "        )\n",
    "        action_weight_output = layers.Dense(\n",
    "            1, name=\"action_weight_output\", bias_initializer=\"ones\"\n",
    "        )\n",
    "\n",
    "        out = hidden_layer(cur_pos)\n",
    "        action_values = action_value_output(out)\n",
    "        action_weights = action_weight_output(out)\n",
    "\n",
    "        self.base_model = tf.keras.Model(\n",
    "            [cur_pos], [action_values, action_weights])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89c0bf",
   "metadata": {},
   "source": [
    "## HallwayEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cb11b",
   "metadata": {},
   "source": [
    "TODO:  HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db058cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
